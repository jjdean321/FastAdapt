<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Fast Adaptation</title>
<style type="text/css">
<!--  body {-->
<!--  margin: 0  auto；-->
<!--  padding:10px 30px;-->
<!--  background: #fff;-->
<!--  color: #111;-->
<!--  font-size: 15px;-->
<!--  font-family: "Times New Roman", serif;-->
<!--  font-weight: 400;-->
<!--  line-height: 1.8;-->
<!--  -webkit-font-smoothing: antialiased;-->
<!--  }-->
  audio {
  width: 75%;
  height:30px;
  }
  img {
  width:100%;
  }
  video {
  width:75%;
  }
  html{
  height:100%;
  }
  body{
  margin: 0  auto；
  padding:10px 30px;
  background: #fff;
  color: #111;
  height:100%;
  font-size: 17px;
  font-family: "Times New Roman", serif;
  font-weight: 400;
  line-height: 1.8;
  overflow-x: hidden;
  -webkit-font-smoothing: antialiased;
  }
</style>
</head>
<div style="border: none; width:80%; margin: 0 auto;">
<body>
    <h2 align="center">Towards High-Performance and Low-Latency Feature-Based Speaker Adaptation of Conformer Speech Recognition Systems</h2>
    <a href="#sectionI">I. Experimental Setup and Results</a><br>
    <a href="#sectionII">II. Reference</a>
  
    <h3> <a name="sectionI">I. Experimental Setup and Results</a></h3>
    <div style="border: none; width:100%; margin: 0 auto;">
    <h4> A. Experimental setup </h4>
        <ul>
            <li> The proposed feature-based subspace LHUC adaptation approaches are investigated on the ESPnet recipe configured Conformer ASR systems trained on the 300-hr Switchboard corpus.  </li>
            <li> T-SNE visualization of the proposed f-Sub-LHUC feature, i-vector feature, and x-vector feature on the 4-hrs Hub5'00 set (80 speakers).  </li>
        </ul>
    </div>
    <h4> B. T-SNE Visualization of All Speakers on the Hub5'00 Set </h4>
    <div style="border: none; width:80%; margin: 0 auto;">
    <table border="1">
      <tr>
        <td><img src="Results/fig3_versus_en_4156-A.png" alt> </td>
        <td><img src="Results/fig3_versus_en_4156-B.png" alt></td>
        <td><img src="Results/fig3_versus_en_4170-A.png" alt></td>
        <td><img src="Results/fig3_versus_en_4170-B.png" alt></td>
      </tr>
      <tr>
        <td><img src="Results/fig3_versus_en_4183-A.png" alt></td>
        <td><img src="Results/fig3_versus_en_4183-B.png" alt></td>
        <td><img src="Results/fig3_versus_en_4404-A.png" alt></td>
        <td><img src="Results/fig3_versus_en_4404-B.png" alt></td>
      </tr>
      <tr>
        <td><img src="Results/fig3_versus_en_4574-A.png" alt></td>
        <td><img src="Results/fig3_versus_en_4574-B.png" alt></td>
        <td><img src="Results/fig3_versus_en_4616-A.png" alt></td>
        <td><img src="Results/fig3_versus_en_4616-B.png" alt></td>
      </tr>
      <tr>
        <td><img src="Results/fig3_versus_en_4622-A.png" alt></td>
        <td><img src="Results/fig3_versus_en_4622-B.png" alt></td>
        <td><img src="Results/fig3_versus_en_4852-A.png" alt></td>
        <td><img src="Results/fig3_versus_en_4852-B.png" alt></td>
      </tr>
      <tr>
        <td><img src="Results/fig3_versus_en_4910-A.png" alt></td>
        <td><img src="Results/fig3_versus_en_4910-B.png" alt></td>
        <td><img src="Results/fig3_versus_en_4938-A.png" alt></td>
        <td><img src="Results/fig3_versus_en_4938-B.png" alt></td>
      </tr>
      <tr>
        <td><img src="Results/fig3_versus_en_4966-A.png" alt></td>
        <td><img src="Results/fig3_versus_en_4966-B.png" alt></td>
        <td><img src="Results/fig3_versus_en_5011-A.png" alt></td>
        <td><img src="Results/fig3_versus_en_5011-B.png" alt></td>
      </tr>
    </table>
    </div>
<h3><a name="sectionII">II. Reference</a></h3>
<p> [1] J. Yu et al., “Audio-visual multi-channel recognition of overlapped speech,” in INTERSPEECH, 2020, pp. 3496–3500.</p>

</body>
</div>
</html>
