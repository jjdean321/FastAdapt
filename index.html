<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Fast Adaptation</title>
<style type="text/css">
<!--  body {-->
<!--  margin: 0  auto；-->
<!--  padding:10px 30px;-->
<!--  background: #fff;-->
<!--  color: #111;-->
<!--  font-size: 15px;-->
<!--  font-family: "Times New Roman", serif;-->
<!--  font-weight: 400;-->
<!--  line-height: 1.8;-->
<!--  -webkit-font-smoothing: antialiased;-->
<!--  }-->
  audio {
  width: 75%;
  height:30px;
  }
  img {
  width:100%;
  }
  video {
  width:75%;
  }
  html{
  height:100%;
  }
  body{
  margin: 0  auto；
  padding:10px 30px;
  background: #fff;
  color: #111;
  height:100%;
  font-size: 17px;
  font-family: "Times New Roman", serif;
  font-weight: 400;
  line-height: 1.8;
  overflow-x: hidden;
  -webkit-font-smoothing: antialiased;
  }
</style>
</head>
<div style="border: none; width:80%; margin: 0 auto;">
<body>
    <h2 align="center">Towards High-Performance and Low-Latency Feature-Based Speaker Adaptation of Conformer Speech Recognition Systems</h2>
    <a href="#sectionI">I. Experimental Setup and Results</a><br>
    <a href="#sectionII">II. Reference</a>
  
    <h3> <a name="sectionI">I. Experimental Setup and Results</a></h3>
    <div style="border: none; width:100%; margin: 0 auto;">
    <h4> A. Experimental setup </h4>
        <ul>
            <li> A 15-channel symmetric linear array with non-even inter-channel spacing is leveraged to simulate the multi-channel overlapped-reverberant-noisy mixture speech using the Oxford LRS2 dataset [3] with 96997, 4272 and 4972 utterances respectively for training (91.37 hours), validation (2.59 hours) and test (2.32 hours). </li>
            <li> 1200 (0.5 hours) utterances are recorded by replaying two loudspeakers simultaneously to generate a 15-channel overlapped-reverberant-noisy mixture speech in a meeting room [1], based on the Oxford LRS2 test set. The geometric specification of the microphone array used during recording is the same as that used in the simulation. </li>
            <li> Espnet style Conformer AED ASR model [4] contains 12 encoder and 6 decoder layers is used in the submitted paper. </li>
        </ul>
    </div>
    <h4> B. T-SNE Results of All Speakers on the Hub5'00 Set </h4>
    <div style="border: none; width:80%; margin: 0 auto;">
    <table border="1">
      <tr>
        <td><img src="Results/fig3_versussw_4910-B.png" alt> </td>
        <td><img src="Results/fig3_versussw_4910-B.png" alt></td>
        <td><img src="Results/fig3_versussw_4910-B.png" alt></td>
      </tr>
      <tr>
        <td><img src="Results/fig3_versussw_4910-B.png" alt></td>
        <td><img src="Results/fig3_versussw_4910-B.png" alt></td>
        <td><img src="Results/fig3_versussw_4910-B.png" alt></td>
      </tr>
    </table>
    </div>
<h3><a name="sectionIV">IV. Reference</a></h3>
<p> [1] J. Yu et al., “Audio-visual multi-channel recognition of overlapped speech,” in INTERSPEECH, 2020, pp. 3496–3500.</p>
<p> [2] G. Li et al., “Audio-visual multi-channel speech separation, dereverberation and recognition,” in ICASSP, 2022, pp. 6042–6046.</p>
<p> [3] J. Son Chung et al., “Lip reading sentences in the wild,” in IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 6447–6456. </p>
<p> [4] A. Gulati et al., “Conformer: Convolution-augmented transformer for speech recognition,” in INTERSPEECH, 2020, pp. 5036–5040</p>


</body>
</div>
</html>
